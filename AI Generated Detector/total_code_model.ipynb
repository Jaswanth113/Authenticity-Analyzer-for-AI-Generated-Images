{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from PIL import ImageChops, ImageEnhance  # Adding missing imports from PIL\n",
    "import os\n",
    "%matplotlib inline\n",
    "np.random.seed(2)\n",
    "import keras\n",
    "import optimizer\n",
    "from keras import regularizers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Conv2D, MaxPool2D, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "from PIL import Image, ImageChops, ImageEnhance\n",
    "import os\n",
    "import itertools\n",
    "\n",
    "def convert_to_ela_image(path, quality):\n",
    "    temp_filename = 'temp_file_name.jpg'\n",
    "    ela_filename = 'temp_ela.jpg'\n",
    "    \n",
    "    image = Image.open(path).convert('RGB')\n",
    "    image.save(temp_filename, 'JPEG', quality = quality)\n",
    "    temp_image = Image.open(temp_filename)\n",
    "    \n",
    "    ela_image = ImageChops.difference(image, temp_image)\n",
    "    \n",
    "    extrema = ela_image.getextrema()\n",
    "    max_diff = max([ex[1] for ex in extrema])\n",
    "    if max_diff == 0:\n",
    "        max_diff = 1\n",
    "    scale = 255.0 / max_diff\n",
    "    \n",
    "    ela_image = ImageEnhance.Brightness(ela_image).enhance(scale)\n",
    "    \n",
    "    return ela_image\n",
    "\n",
    "real_image_path = 'archive (2)\\casia\\CASIA2\\Au\\Au_ani_00001.jpg'\n",
    "Image.open(real_image_path)\n",
    "\n",
    "convert_to_ela_image(real_image_path, 90)\n",
    "\n",
    "fake_image_path = 'archive (2)\\casia\\CASIA2\\Tp\\Tp_D_NRN_S_N_ani10171_ani00001_12458.jpg'\n",
    "Image.open(fake_image_path)\n",
    "\n",
    "convert_to_ela_image(fake_image_path, 90)\n",
    "\n",
    "image_size = (128, 128)\n",
    "\n",
    "def prepare_image(image_path):\n",
    "    return np.array(convert_to_ela_image(image_path, 90).resize(image_size)).flatten() /255.0\n",
    "\n",
    "X = [] # ELA converted images\n",
    "Y = [] # 0 for fake, 1 for real\n",
    "\n",
    "import random\n",
    "path = 'archive (2)\\CASIA2\\Au'\n",
    "for dirname, _, filenames in os.walk(path):\n",
    "    for filename in filenames:\n",
    "        if filename.endswith('jpg') or filename.endswith('tif'):\n",
    "            full_path = os.path.join(dirname, filename)\n",
    "            X.append(prepare_image(full_path))\n",
    "            Y.append(1)\n",
    "            if len(Y) % 500 == 0:\n",
    "                print(f'Processing {len(Y)} images')\n",
    "                \n",
    "print(len(X), len(Y))\n",
    "\n",
    "path = 'archive (2)\\CASIA2\\Tp'\n",
    "for dirname, _, filenames in os.walk(path):\n",
    "    for filename in filenames:\n",
    "        if filename.endswith('jpg') or filename.endswith('tif'):\n",
    "            full_path = os.path.join(dirname, filename)\n",
    "            X.append(prepare_image(full_path))\n",
    "            Y.append(0)\n",
    "            if len(Y) % 500 == 0:\n",
    "                print(f'Processing {len(Y)} images')\n",
    "\n",
    "print(len(X), len(Y))\n",
    "\n",
    "X = np.array(X)\n",
    "Y = to_categorical(Y, 2)\n",
    "X = X.reshape(-1, 128, 128, 3)\n",
    "\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size = 0.2, random_state=5)\n",
    "print(len(X_train), len(Y_train))\n",
    "print(len(X_val), len(Y_val))\n",
    "\n",
    "def build_model():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(filters=32, kernel_size=(5, 5), padding='valid', activation='relu', input_shape=(128, 128, 3)))\n",
    "    model.add(Conv2D(filters=32, kernel_size=(5, 5), padding='valid', activation='relu', kernel_regularizer=regularizers.l2(0.01)))  # Example of weight decay regularization\n",
    "    model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.01)))  # Example of weight decay regularization\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    return model\n",
    "\n",
    "model = build_model()\n",
    "model.summary()\n",
    "\n",
    "# Training parameters\n",
    "epochs = 35\n",
    "batch_size = 32\n",
    "init_lr = 1e-4\n",
    "\n",
    "# Compile the model with Adam optimizer\n",
    "optimizer = Adam(learning_rate=init_lr)\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "early_stopping = EarlyStopping(monitor = 'val_acc',\n",
    "                              min_delta = 0.05,\n",
    "                              patience = 20,\n",
    "                              verbose = 0,\n",
    "                              mode = 'auto')\n",
    "\n",
    "hist = model.fit(X_train,\n",
    "                 Y_train,\n",
    "                 batch_size = batch_size,\n",
    "                 epochs = epochs,\n",
    "                validation_data = (X_val, Y_val),\n",
    "                callbacks = [early_stopping])\n",
    "\n",
    "model.save('model_casia_run.h5')\n",
    "\n",
    "# Plot the loss and accuracy curves for training and validation \n",
    "fig, ax = plt.subplots(2, 1, figsize=(10, 10))\n",
    "\n",
    "# Plot training and validation loss\n",
    "ax[0].plot(hist.history['loss'], color='b', label=\"Training loss\")\n",
    "ax[0].plot(hist.history['val_loss'], color='r', label=\"Validation loss\")\n",
    "ax[0].legend(loc='best', shadow=True)\n",
    "\n",
    "# Plot training and validation accuracy\n",
    "ax[1].plot(hist.history['accuracy'], color='b', label=\"Training accuracy\")\n",
    "ax[1].plot(hist.history['val_accuracy'], color='r', label=\"Validation accuracy\")\n",
    "ax[1].legend(loc='best', shadow=True)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "# Predict the values from the validation dataset\n",
    "Y_pred = model.predict(X_val)\n",
    "# Convert predictions classes to one hot vectors \n",
    "Y_pred_classes = np.argmax(Y_pred,axis = 1) \n",
    "# Convert validation observations to one hot vectors\n",
    "Y_true = np.argmax(Y_val,axis = 1) \n",
    "# compute the confusion matrix\n",
    "confusion_mtx = confusion_matrix(Y_true, Y_pred_classes) \n",
    "# plot the confusion matrix\n",
    "plot_confusion_matrix(confusion_mtx, classes = range(2))\n",
    "\n",
    "class_names = ['fake', 'real']\n",
    "\n",
    "real_image_path = 'archive (2)/CASIA1/Sp/Sp_D_NND_A_sec0067_ani0096_0620.jpg'\n",
    "image = prepare_image(real_image_path)\n",
    "image = image.reshape(-1, 128, 128, 3)\n",
    "y_pred = model.predict(image)\n",
    "y_pred_class = np.argmax(y_pred, axis = 1)[0]\n",
    "print(f'Class: {class_names[y_pred_class]} Confidence: {np.amax(y_pred) * 100:0.2f}')\n",
    "\n",
    "fake_image_path = 'archive (2)/casia/CASIA2/Tp/Tp_D_NRN_S_N_ani10171_ani00001_12458.jpg'\n",
    "image = prepare_image(fake_image_path)\n",
    "image = image.reshape(-1, 128, 128, 3)\n",
    "y_pred = model.predict(image)\n",
    "y_pred_class = np.argmax(y_pred, axis = 1)[0]\n",
    "print(f'Class: {class_names[y_pred_class]} Confidence: {np.amax(y_pred) * 100:0.2f}')\n",
    "\n",
    "fake_image = os.listdir('archive (2)/casia/CASIA2/Tp/')\n",
    "correct = 0\n",
    "total = 0\n",
    "for file_name in fake_image:\n",
    "    if file_name.endswith('jpg') or file_name.endswith('tif'):\n",
    "        fake_image_path = os.path.join('archive (2)/casia/CASIA2/Tp/', file_name)\n",
    "        image = prepare_image(fake_image_path)\n",
    "        image = image.reshape(-1, 128, 128, 3)\n",
    "        y_pred = model.predict(image)\n",
    "        y_pred_class = np.argmax(y_pred, axis = 1)[0]\n",
    "        total += 1\n",
    "        if y_pred_class == 0:\n",
    "            correct += 1\n",
    "        print(f'Class: {class_names[y_pred_class]} Confidence: {np.amax(y_pred) * 100:0.2f}')\n",
    "\n",
    "print(f'Total: {total}, Correct: {correct}, Acc: {correct / total * 100.0}')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
